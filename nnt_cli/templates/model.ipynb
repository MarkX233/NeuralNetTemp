{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] D:\\SoftProject\\Python\\SNN_demo\\QSNN_NMNIST\\L4_no_re.ipynb: Notebook name is correct.\n",
            "[OK] D:\\SoftProject\\Python\\SNN_demo\\QSNN_NMNIST\\L4_re.ipynb: Notebook name is correct.\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "import snntorch as snn\n",
        "from snntorch import functional as SF\n",
        "\n",
        "\n",
        "import nnt_cli.utils as nu\n",
        "from pro_temp import _Project_Template\n",
        "\n",
        "\n",
        "\n",
        "notebook_path = Path().resolve()\n",
        "parent_path = os.path.join(notebook_path, \"..\")\n",
        "sys.path.append(parent_path)\n",
        "\n",
        "notebook_name=\"\"\n",
        "nu.settin.gen_settin.get_notebook_name(notebook_path)\n",
        "notebook_name='L4_no_re'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Parameters in this cell can be changed using script.\n",
        "# But you have to pass these parameters to the model class. \n",
        "script_mode = False\n",
        "# Use script_mode to use separate settings between using script and direct running.\n",
        "debug_mode = True\n",
        "\n",
        "batch_size = 256\n",
        "num_epochs = 1\n",
        "lr = 1e-3\n",
        "\n",
        "beta = 0.95\n",
        "bit_width=[8,4,2,1]\n",
        "num_hiddens=1024\n",
        "\n",
        "# num_steps=1000\n",
        "\n",
        "# `match_name` and `remark` will be used to generate the file name of the saved model.\n",
        "# When using script, the name set here will be used.\n",
        "# It's better for `match_name` to fit variable setting in script.\n",
        "match_name=f\"{notebook_name}_bit_width_\"\n",
        "remark=f\"4_linear_layers\"\n",
        "\n",
        "learn_beta_threshold=True\n",
        "\n",
        "train_method = \"one\"\n",
        "# \"one\": Train for one time.\n",
        "# \"iter\": Train in one iteration, like sweep.\n",
        "# \"diter\": Train in two nested iterations. \n",
        "# The settings of iteration are in the method `set_iter`.\n",
        "\n",
        "reset_mode=[1,1,1,1]\n",
        "# Reset mode for Leaky layer.\n",
        "# 0 is \"zero\", 1 is \"subtract\"\n",
        "# Index of number is the index of Leaky layer.\n",
        "\n",
        "cp_fpath=\"\"\n",
        "# checkpoint path\n",
        "\n",
        "random_seed=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use class internal `set_name` method, when manually run the notebook.\n",
        "# For debug\n",
        "if script_mode is False:\n",
        "    match_name=None\n",
        "    remark=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# papermill may inject list parameter as str.\n",
        "# Here is a example of bit_width setting.\n",
        "\n",
        "import ast\n",
        "if isinstance(bit_width, str):\n",
        "    try:\n",
        "        bit_width = ast.literal_eval(bit_width)\n",
        "    except (SyntaxError, ValueError) as e:\n",
        "        raise ValueError(\"Wrong format, can't transform it to list.\") from e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class _Project_Run(_Project_Template):\n",
        "    def set_name(self):\n",
        "        # Default name setting of results.\n",
        "        self.match_name=f\"{self.notebook_name}_qsnn_NMNIST_manual_\"\n",
        "        self.remark=f\"4_linear_layers\"\n",
        "\n",
        "    def init_params(self):\n",
        "        \"\"\"\n",
        "        **Essential**\n",
        "        Set the global parameters for the class, including settings.\n",
        "        Suggestion: Set init parameters here that are not necessary to be changed by script.\n",
        "        \"\"\"\n",
        "\n",
        "        super().init_params()\n",
        "\n",
        "        self.infer_size=0.01\n",
        "        # Size of NMNIST test dataset is 10000\n",
        "        # Because the quantized inference dataflow will be saved, a large inference dataset will significantly increase the time of training. \n",
        "        # In HPC 1 sample of data may costs 3.42 MB and 3 min to write.\n",
        "\n",
        "        self.with_cache=\"False\"\n",
        "\n",
        "        self.quick_debug=False\n",
        "        self.qb_dataset_size=0.1\n",
        "\n",
        "        # self.sav_state = False\n",
        "        # self.sav_checkpoint = False\n",
        "\n",
        "        self.weight_decay=0.01\n",
        "        # For optimizer\n",
        "\n",
        "        self.dropout_prob=0.2\n",
        "    \n",
        "    def set_iter(self):\n",
        "        \"\"\"\n",
        "        Set the iteration variables and values.\n",
        "        The variables set here will be used for iteration training if you call the iteration training methods.\n",
        "        `self.variable_name` will be used first when `iter` (sweep) mode is called.\n",
        "        `self.variable_name` must be consistent with the variable names in the class defined.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # self.vary_list=[\n",
        "        #     [8,8,8,8],\n",
        "        #     [4,4,4,4],\n",
        "        #     [2,2,2,2],\n",
        "        #     [1,1,1,1],\n",
        "        # ]\n",
        "        # self.variable_name=\"bit_width\"\n",
        "        # # Must be consistent with the variable names in the class defined.\n",
        "\n",
        "        # self.vary_list2=[1024,2048]\n",
        "        # self.variable_name2=\"num_hiddens\"\n",
        "        # # The other variables, bit_width, are set by script, so the full performance of HPC could be used.  \n",
        "    \n",
        "    def set_model(self):\n",
        "        \"\"\"\n",
        "        **Essential**\n",
        "        Set the training model.\n",
        "        `self.net`, `self.loss`, `self.optimizer` must be set.\n",
        "        In subclass, do NOT use super() to inherit optimizer!!!\n",
        "        \"\"\"\n",
        "        # super().set_model()\n",
        "        # DO NOT USE super() to inherit optimizer!!!\n",
        "\n",
        "        \n",
        "        # num_inputs =2*34*34\n",
        "        # num_hiddens=self.num_hiddens\n",
        "        # num_outputs = 10\n",
        "\n",
        "        # act_quant=nu.settin.qnn_settin.get_act_quant(self.bit_width[0])\n",
        "        # weight_quant=[nu.settin.qnn_settin.get_weight_quant(bw) for bw in self.bit_width]\n",
        "\n",
        "        # reset_mode_str=self.decode_reset_mode(self.reset_mode)\n",
        "        # self.net = nn.Sequential(\n",
        "        #         # unnamed layer, use layer1.parameters() or layer1.weight to access\n",
        "        #         # named layer, use net.<name>.param to access\n",
        "        #         OrderedDict([\n",
        "        #             ('flaten', nn.Flatten()), # Already flatted in the dataset.\n",
        "        #             # ('input_sav0', du.InputSaviorLayer(f\"{self.sav_data_path}/{self.vary_title}\",\"input_bf_quant\",squeeze=True)),\n",
        "        #             ('quant_ident', qnn.QuantIdentity(act_quant=act_quant,return_quant_tensor=True,\n",
        "        #                                             )),\n",
        "        #             ('input_sav1', du.InputSaviorLayer(f\"{self.sav_data_path}/{self.vary_title}\",\"input_af_quant\",squeeze=True)),\n",
        "\n",
        "        #             ('quant_linear_in', qnn.QuantLinear(num_inputs, num_hiddens,bias=False,device=self.device,\n",
        "        #                                             weight_quant=weight_quant[0],)), \n",
        "        #             ('dropout1', nn.Dropout(self.dropout_prob)),                               \n",
        "        #             ('batch_norm1', nn.BatchNorm1d(num_hiddens)),\n",
        "        #             ('leaky1', snn.Leaky(beta=self.beta, init_hidden=True,reset_mechanism=reset_mode_str[0], \n",
        "        #                                  learn_beta=self.learn_beta_threshold, learn_threshold=self.learn_beta_threshold)),\n",
        "                    \n",
        "        #             ('quant_linear2', qnn.QuantLinear(num_hiddens, num_hiddens,bias=False,device=self.device,\n",
        "        #                                             weight_quant=weight_quant[1],)),  \n",
        "        #             ('dropout2', nn.Dropout(self.dropout_prob)),                              \n",
        "        #             ('batch_norm2', nn.BatchNorm1d(num_hiddens)),\n",
        "        #             ('leaky2', snn.Leaky(beta=self.beta, init_hidden=True,reset_mechanism=reset_mode_str[1], \n",
        "        #                                  learn_beta=self.learn_beta_threshold, learn_threshold=self.learn_beta_threshold)),\n",
        "\n",
        "        #             ('quant_linear3', qnn.QuantLinear(num_hiddens, num_hiddens,bias=False,device=self.device,\n",
        "        #                                             weight_quant=weight_quant[2],)),\n",
        "        #             ('dropout3', nn.Dropout(self.dropout_prob)),  \n",
        "        #             ('batch_norm3', nn.BatchNorm1d(num_hiddens)),\n",
        "        #             ('leaky3', snn.Leaky(beta=self.beta, init_hidden=True,reset_mechanism=reset_mode_str[2], \n",
        "        #                                  learn_beta=self.learn_beta_threshold, learn_threshold=self.learn_beta_threshold)),\n",
        "\n",
        "        #             ('quant_linear_out',qnn.QuantLinear(num_hiddens, num_outputs,bias=False,device=self.device,\n",
        "        #                                             weight_quant=weight_quant[3],)),\n",
        "        #             ('leaky_out', snn.Leaky(beta=self.beta, init_hidden=True, output=True, reset_mechanism=reset_mode_str[3], \n",
        "        #                                  learn_beta=self.learn_beta_threshold, learn_threshold=self.learn_beta_threshold))\n",
        "        #         ])\n",
        "        #     ).to(self.device)\n",
        "        \n",
        "        # self.loss=SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
        "\n",
        "        # self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=self.learning_rate, betas=(0.9, 0.999), weight_decay=self.weight_decay)\n",
        "\n",
        "        # self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        #     self.optimizer,\n",
        "        #     max_lr=self.learning_rate,\n",
        "        #     total_steps=self.num_epochs,\n",
        "        #     pct_start=0.3,\n",
        "        #     anneal_strategy='cos'\n",
        "        # )\n",
        "    \n",
        "    def init_net(self):\n",
        "        \"\"\"\n",
        "        Init net if you need to.\n",
        "        \"\"\"\n",
        "        # print(\"No init of the network!\")\n",
        "        # # No init, because I'm not sure how to init the quantized layer.\n",
        "    \n",
        "\n",
        "run_class_i=_Project_Run(notebook_name,script_mode,debug_mode,batch_size,num_epochs,lr,\n",
        "                     beta,bit_width,num_hiddens,reset_mode,match_name=match_name,remark=remark,\n",
        "                     learn_beta_threshold=learn_beta_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For better representative, you can manually set seed:\n",
        "run_class_i.set_seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You can use this block to check dataset\n",
        "\n",
        "# run_class_i.init_params()\n",
        "# run_class_i.set_path()\n",
        "# run_class_i.set_dataset()\n",
        "# run_class_i.set_dataloader()\n",
        "\n",
        "# frame,_ = next(iter(run_class_i.train_loader))\n",
        "\n",
        "# print(frame.shape )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check dataset by iterating all dataset to find out whether there is a store/cache error.\n",
        "# You can also use this to set cache, if you have set up cache mode first.\n",
        "\n",
        "# run_class_i.check_dataset()\n",
        "\n",
        "\n",
        "# # If cache has error, use this to clear the cache.\n",
        "# run_class_i.remove_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A interior method to analyze dataset\n",
        "\n",
        "# run_class_i.dataset_analyze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A visualize method to call multi visual functions to analyze dataset\n",
        "\n",
        "# run_class_i.get_visual_frame_distribution(quantile=0.98)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting of the training method\n",
        "# \"one\": Train for one time.\n",
        "# \"iter\": Train in one iteration, like sweep. Only use run_class_i.variable_name and its values to do iteration training.\n",
        "# \"diter\": Train in two nested iterations. \n",
        "# \"cp\": Load from checkpoint. And the model will start form that state.\n",
        "\n",
        "match train_method:\n",
        "    case \"one\":\n",
        "        run_class_i.train_onetime(no_plot=True)\n",
        "    case \"iter\":\n",
        "        run_class_i.train_iter(no_plot=True)\n",
        "    case \"diter\":\n",
        "        run_class_i.train_double_iter(no_plot=True)\n",
        "    case \"cp\":\n",
        "        run_class_i.load_checkpoint(cp_fpath,no_plot=True)\n",
        "    case _:\n",
        "        raise ValueError(\"Invalid train method!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_class_i.plot_final()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_class_i.plot_record()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "QSNN example training with QuantTensor=True, \n",
        "Test different mix-precision for the QNN and QSNN 1,2,4\n",
        "Floating point SNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze after training\n",
        "\n",
        "run_class_i.check_gradient_norm()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py312-cuda121-pt24-tt018",
      "language": "python",
      "name": "py312-cuda121-pt24-tt018"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
